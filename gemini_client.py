"""
–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API (Google AI)
–ù–æ–≤—ã–π SDK: google.genai
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://ai.google.dev/gemini-api/docs
"""
import asyncio
import json
import os
from typing import AsyncGenerator
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
LEVEL_PROMPTS = {
    "child": """–¢—ã ‚Äî –¥–æ–±—Ä—ã–π —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫ –¥–ª—è –¥–µ—Ç–µ–π. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É —Ç–∞–∫, —á—Ç–æ–±—ã –ø–æ–Ω—è–ª 5-–ª–µ—Ç–Ω–∏–π —Ä–µ–±—ë–Ω–æ–∫:
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞
- –ü—Ä–∏–≤–æ–¥–∏ —è—Ä–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∂–∏–∑–Ω–∏ —Ä–µ–±—ë–Ω–∫–∞ (–∏–≥—Ä—É—à–∫–∏, –∂–∏–≤–æ—Ç–Ω—ã–µ, –µ–¥–∞)
- –û–±—ä—è—Å–Ω—è–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
- –î–æ–±–∞–≤—å –Ω–µ–º–Ω–æ–≥–æ –≤–µ—Å–µ–ª—å—è –∏ —ç–Ω—Ç—É–∑–∏–∞–∑–º–∞
- –ú–∞–∫—Å–∏–º—É–º 200 —Å–ª–æ–≤""",

    "school": """–¢—ã ‚Äî —à–∫–æ–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É –¥–ª—è —É—á–µ–Ω–∏–∫–∞ 10-12 –∫–ª–∞—Å—Å–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π —à–∫–æ–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
- –û–±—ä—è—Å–Ω—è–π –ª–æ–≥–∏–∫—É –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–æ–Ω—è—Ç–∏—è–º–∏
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
- 2-3 –∞–±–∑–∞—Ü–∞""",

    "student": """–¢—ã ‚Äî –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞ 2-3 –∫—É—Ä—Å–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –†–∞—Å–∫—Ä—ã–≤–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –û–±—ä—è—Å–Ω—è–π –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã
- –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç""",

    "expert": """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±–ª–∞—Å—Ç–∏. –î–∞–π —É–≥–ª—É–±–ª—ë–Ω–Ω–æ–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:
- –ò—Å–ø–æ–ª—å–∑—É–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é –∏ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –†–∞—Å–∫—Ä—ã–≤–∞–π –Ω—é–∞–Ω—Å—ã, edge cases –∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã
- –£–ø–æ–º—è–Ω–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ best practices
- –û–±—Å—É–¥–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞""",
}

LEVEL_NAMES = {
    "child": "üë∂ 5-–ª–µ—Ç–Ω–∏–π —Ä–µ–±—ë–Ω–æ–∫",
    "school": "üéí –®–∫–æ–ª—å–Ω–∏–∫",
    "student": "üéì –°—Ç—É–¥–µ–Ω—Ç",
    "expert": "üî¨ –≠–∫—Å–ø–µ—Ä—Ç",
}

# –ú–∞–ø–ø–∏–Ω–≥ —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ
LEVEL_TEMPERATURE = {
    "child": 0.9,
    "school": 0.7,
    "student": 0.6,
    "expert": 0.5,
}


class GeminiClient:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or GEMINI_API_KEY
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å GEMINI_API_KEY=your_key")
        
        # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç —Å API –∫–ª—é—á–æ–º
        self.client = genai.Client(api_key=self.api_key)
        
    def _build_system_prompt(self, level: str, format_description: str, explicit_stop: bool, stop_sequence: str) -> str:
        """–°–æ–±–∏—Ä–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á—ë—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫."""
        parts = [LEVEL_PROMPTS[level]]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
        if format_description:
            parts.append(f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞:\n{format_description}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        if explicit_stop:
            if stop_sequence:
                parts.append(f"\n\n–í–ê–ñ–ù–û: –ó–∞–∫–æ–Ω—á–∏ —Å–≤–æ–π –æ—Ç–≤–µ—Ç —Ñ—Ä–∞–∑–æ–π \"{stop_sequence}\".")
            else:
                parts.append("\n\n–í–ê–ñ–ù–û: –ó–∞–∫–æ–Ω—á–∏ —Å–≤–æ–π –æ—Ç–≤–µ—Ç —á—ë—Ç–∫–æ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é. –ù–µ –æ–±—Ä—ã–≤–∞–π –º—ã—Å–ª—å –Ω–∞ –ø–æ–ª—É—Å–ª–æ–≤–µ.")
        
        return "\n".join(parts)
    
    def _get_config(self, level: str, max_tokens: int, stop_sequence: str) -> types.GenerateContentConfig:
        """–°–æ–∑–¥–∞—ë—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        temperature = LEVEL_TEMPERATURE.get(level, 0.7)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        stop_sequences = []
        if stop_sequence:
            stop_sequences.append(stop_sequence)
        
        return types.GenerateContentConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
            stop_sequences=stop_sequences if stop_sequences else None,
        )
    
    async def explain(
        self, 
        topic: str, 
        level: str,
        format_description: str = "",
        max_tokens: int = 2000,
        stop_sequence: str = "",
        explicit_stop: bool = True,
    ) -> dict:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–º—ã.
        
        Args:
            topic: –¢–µ–º–∞ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            level: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (child, school, student, expert)
            format_description: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            stop_sequence: –°—Ç–æ–ø-—Å–ª–æ–≤–æ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            explicit_stop: –î–æ–±–∞–≤–∏—Ç—å —è–≤–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        
        Returns:
            dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞
        """
        if level not in LEVEL_PROMPTS:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {level}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(LEVEL_PROMPTS.keys())}")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = self._build_system_prompt(level, format_description, explicit_stop, stop_sequence)
        user_prompt = f"–û–±—ä—è—Å–Ω–∏ —Ç–µ–º—É: {topic}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π "—Å—ã—Ä–æ–π" –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        raw_request = {
            "model": "gemini-3-flash-preview",
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": f"{system_prompt}\n\n{user_prompt}"}]
                }
            ],
            "config": {
                "temperature": LEVEL_TEMPERATURE.get(level, 0.7),
                "max_output_tokens": max_tokens,
                "stop_sequences": [stop_sequence] if stop_sequence else [],
            }
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        settings = {
            "format_description": format_description,
            "max_tokens": max_tokens,
            "stop_sequence": stop_sequence,
            "explicit_stop": explicit_stop,
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –∑–∞–ø—Ä–æ—Å
        print("\n" + "=" * 60)
        print("üî¥ –û–¢–ü–†–ê–í–ö–ê –ó–ê–ü–†–û–°–ê –í GEMINI API")
        print("=" * 60)
        print(f"URL: https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent")
        print(f"Method: POST")
        print(f"Headers: {{")
        print(f'  "Authorization": "Bearer {self.api_key[:10]}...{self.api_key[-4:]}",')
        print(f'  "Content-Type": "application/json"')
        print(f"}}")
        print("-" * 60)
        print("Body:")
        print(json.dumps(raw_request, ensure_ascii=False, indent=2))
        print("=" * 60)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config = self._get_config(level, max_tokens, stop_sequence)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–∞–ø—Ä–æ—Å–∞
            contents = f"{system_prompt}\n\n{user_prompt}"
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop)
            response = await asyncio.to_thread(
                self.client.models.generate_content,
                model="gemini-3-flash-preview",
                contents=contents,
                config=config,
            )
            
            explanation = response.text
            
            # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –æ—Ç–≤–µ—Ç–∞, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            if stop_sequence and explanation.endswith(stop_sequence):
                explanation = explanation[:-len(stop_sequence)].rstrip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            usage_metadata = response.usage_metadata if hasattr(response, 'usage_metadata') else None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π "—Å—ã—Ä–æ–π" –æ—Ç–≤–µ—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            raw_response = {
                "candidates": [
                    {
                        "content": {
                            "role": "model",
                            "parts": [{"text": explanation}]
                        },
                        "finish_reason": "STOP",
                    }
                ],
                "usage_metadata": {
                    "prompt_token_count": usage_metadata.prompt_token_count if usage_metadata else 0,
                    "candidates_token_count": usage_metadata.candidates_token_count if usage_metadata else 0,
                    "total_token_count": usage_metadata.total_token_count if usage_metadata else 0,
                },
                "model": "gemini-3-flash-preview",
            }
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç
            print("\n" + "=" * 60)
            print("üü¢ –ü–û–õ–£–ß–ï–ù –û–¢–í–ï–¢ –û–¢ GEMINI API")
            print("=" * 60)
            print(json.dumps(raw_response, ensure_ascii=False, indent=2))
            print("=" * 60 + "\n")
            
            return {
                "success": True,
                "explanation": explanation,
                "topic": topic,
                "level": level,
                "level_name": LEVEL_NAMES.get(level),
                "model": "gemini-3-flash-preview",
                "usage": {
                    "prompt_tokens": raw_response["usage_metadata"]["prompt_token_count"],
                    "completion_tokens": raw_response["usage_metadata"]["candidates_token_count"],
                    "total_tokens": raw_response["usage_metadata"]["total_token_count"],
                },
                "settings": settings,
                "raw_request": raw_request,
                "raw_response": raw_response,
            }
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ Gemini API: {e}")
            raise
    
    async def stream_explain(
        self, 
        topic: str, 
        level: str,
        format_description: str = "",
        max_tokens: int = 2000,
        stop_sequence: str = "",
        explicit_stop: bool = True,
    ) -> AsyncGenerator[str, None]:
        """
        –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º.
        """
        if level not in LEVEL_PROMPTS:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {level}")
        
        system_prompt = self._build_system_prompt(level, format_description, explicit_stop, stop_sequence)
        user_prompt = f"–û–±—ä—è—Å–Ω–∏ —Ç–µ–º—É: {topic}"
        
        print(f"\n[STREAM] –ó–∞–ø—Ä–æ—Å: —Ç–µ–º–∞='{topic}', —É—Ä–æ–≤–µ–Ω—å='{level}'\n")
        
        config = self._get_config(level, max_tokens, stop_sequence)
        contents = f"{system_prompt}\n\n{user_prompt}"
        
        # –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        queue = asyncio.Queue()
        
        def generate_stream():
            try:
                for chunk in self.client.models.generate_content_stream(
                    model="gemini-3-flash-preview",
                    contents=contents,
                    config=config,
                ):
                    if chunk.text:
                        asyncio.run_coroutine_threadsafe(queue.put(chunk.text), loop)
            finally:
                asyncio.run_coroutine_threadsafe(queue.put(None), loop)
        
        loop = asyncio.get_event_loop()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        import threading
        thread = threading.Thread(target=generate_stream)
        thread.start()
        
        # –ß–∏—Ç–∞–µ–º chunks –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        while True:
            chunk = await queue.get()
            if chunk is None:
                break
            yield chunk
        
        thread.join()
