"""
–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API (Google AI)
–ù–æ–≤—ã–π SDK: google.genai
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://ai.google.dev/gemini-api/docs
"""
import json
import os
from typing import AsyncGenerator
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
LEVEL_PROMPTS = {
    "child": """–¢—ã ‚Äî –¥–æ–±—Ä—ã–π —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫ –¥–ª—è –¥–µ—Ç–µ–π. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É —Ç–∞–∫, —á—Ç–æ–±—ã –ø–æ–Ω—è–ª 5-–ª–µ—Ç–Ω–∏–π —Ä–µ–±—ë–Ω–æ–∫:
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞
- –ü—Ä–∏–≤–æ–¥–∏ —è—Ä–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∂–∏–∑–Ω–∏ —Ä–µ–±—ë–Ω–∫–∞ (–∏–≥—Ä—É—à–∫–∏, –∂–∏–≤–æ—Ç–Ω—ã–µ, –µ–¥–∞)
- –û–±—ä—è—Å–Ω—è–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
- –î–æ–±–∞–≤—å –Ω–µ–º–Ω–æ–≥–æ –≤–µ—Å–µ–ª—å—è –∏ —ç–Ω—Ç—É–∑–∏–∞–∑–º–∞
- –ú–∞–∫—Å–∏–º—É–º 200 —Å–ª–æ–≤""",

    "school": """–¢—ã ‚Äî —à–∫–æ–ª—å–Ω—ã–π —É—á–∏—Ç–µ–ª—å. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É –¥–ª—è —É—á–µ–Ω–∏–∫–∞ 10-12 –∫–ª–∞—Å—Å–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π —à–∫–æ–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
- –û–±—ä—è—Å–Ω—è–π –ª–æ–≥–∏–∫—É –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–æ–Ω—è—Ç–∏—è–º–∏
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
- 2-3 –∞–±–∑–∞—Ü–∞""",

    "student": """–¢—ã ‚Äî –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—É –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞ 2-3 –∫—É—Ä—Å–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –†–∞—Å–∫—Ä—ã–≤–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –û–±—ä—è—Å–Ω—è–π –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã
- –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç""",

    "expert": """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±–ª–∞—Å—Ç–∏. –î–∞–π —É–≥–ª—É–±–ª—ë–Ω–Ω–æ–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:
- –ò—Å–ø–æ–ª—å–∑—É–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é –∏ –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –†–∞—Å–∫—Ä—ã–≤–∞–π –Ω—é–∞–Ω—Å—ã, edge cases –∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã
- –£–ø–æ–º—è–Ω–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ best practices
- –û–±—Å—É–¥–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞""",
}

LEVEL_NAMES = {
    "child": "üë∂ 5-–ª–µ—Ç–Ω–∏–π —Ä–µ–±—ë–Ω–æ–∫",
    "school": "üéí –®–∫–æ–ª—å–Ω–∏–∫",
    "student": "üéì –°—Ç—É–¥–µ–Ω—Ç",
    "expert": "üî¨ –≠–∫—Å–ø–µ—Ä—Ç",
}

# –ú–∞–ø–ø–∏–Ω–≥ —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º –º–æ–¥–µ–ª–∏
LEVEL_CONFIG = {
    "child": {"temperature": 0.9, "max_tokens": 500},
    "school": {"temperature": 0.7, "max_tokens": 1000},
    "student": {"temperature": 0.6, "max_tokens": 2000},
    "expert": {"temperature": 0.5, "max_tokens": 4000},
}


class GeminiClient:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or GEMINI_API_KEY
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å GEMINI_API_KEY=your_key")
        
        # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç —Å API –∫–ª—é—á–æ–º
        self.client = genai.Client(api_key=self.api_key)
        
    def _get_config(self, level: str) -> types.GenerateContentConfig:
        """–°–æ–∑–¥–∞—ë—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."""
        config = LEVEL_CONFIG.get(level, LEVEL_CONFIG["school"])
        
        return types.GenerateContentConfig(
            temperature=config["temperature"],
            max_output_tokens=config["max_tokens"],
        )
    
    async def explain(self, topic: str, level: str) -> dict:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–º—ã.
        
        Args:
            topic: –¢–µ–º–∞ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            level: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (child, school, student, expert)
        
        Returns:
            dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞
        """
        if level not in LEVEL_PROMPTS:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {level}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(LEVEL_PROMPTS.keys())}")
        
        system_prompt = LEVEL_PROMPTS[level]
        user_prompt = f"–û–±—ä—è—Å–Ω–∏ —Ç–µ–º—É: {topic}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        request_payload = {
            "model": "gemini-3-flash-preview",
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "level": level,
            "generation_config": LEVEL_CONFIG[level],
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π "—Å—ã—Ä–æ–π" –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        raw_request = {
            "model": "gemini-3-flash-preview",
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": f"{system_prompt}\n\n{user_prompt}"}]
                }
            ],
            "config": {
                "temperature": LEVEL_CONFIG[level]["temperature"],
                "max_output_tokens": LEVEL_CONFIG[level]["max_tokens"],
            }
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –∑–∞–ø—Ä–æ—Å
        print("\n" + "=" * 60)
        print("üî¥ –û–¢–ü–†–ê–í–ö–ê –ó–ê–ü–†–û–°–ê –í GEMINI API")
        print("=" * 60)
        print(f"URL: https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent")
        print(f"Method: POST")
        print(f"Headers: {{")
        print(f'  "Authorization": "Bearer {self.api_key[:10]}...{self.api_key[-4:]}",')
        print(f'  "Content-Type": "application/json"')
        print(f"}}")
        print("-" * 60)
        print("Body:")
        print(json.dumps(raw_request, ensure_ascii=False, indent=2))
        print("=" * 60)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —É—Ä–æ–≤–Ω—è
            config = self._get_config(level)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–∞–ø—Ä–æ—Å–∞
            contents = f"{system_prompt}\n\n{user_prompt}"
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, —Ç.–∫. –Ω–æ–≤—ã–π SDK –Ω–µ —Ç—Ä–µ–±—É–µ—Ç async –¥–ª—è generate_content)
            response = self.client.models.generate_content(
                model="gemini-3-flash-preview",
                contents=contents,
                config=config,
            )
            
            explanation = response.text
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            usage_metadata = response.usage_metadata if hasattr(response, 'usage_metadata') else None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            response_data = {
                "text": explanation[:500] + "..." if len(explanation) > 500 else explanation,
                "prompt_token_count": usage_metadata.prompt_token_count if usage_metadata else 0,
                "candidates_token_count": usage_metadata.candidates_token_count if usage_metadata else 0,
                "total_token_count": usage_metadata.total_token_count if usage_metadata else 0,
            }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π "—Å—ã—Ä–æ–π" –æ—Ç–≤–µ—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            raw_response = {
                "candidates": [
                    {
                        "content": {
                            "role": "model",
                            "parts": [{"text": explanation}]
                        },
                        "finish_reason": "STOP",
                    }
                ],
                "usage_metadata": {
                    "prompt_token_count": response_data["prompt_token_count"],
                    "candidates_token_count": response_data["candidates_token_count"],
                    "total_token_count": response_data["total_token_count"],
                },
                "model": "gemini-3-flash-preview",
            }
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç
            print("\n" + "=" * 60)
            print("üü¢ –ü–û–õ–£–ß–ï–ù –û–¢–í–ï–¢ –û–¢ GEMINI API")
            print("=" * 60)
            print(json.dumps(raw_response, ensure_ascii=False, indent=2))
            print("=" * 60 + "\n")
            
            return {
                "success": True,
                "explanation": explanation,
                "topic": topic,
                "level": level,
                "level_name": LEVEL_NAMES.get(level),
                "model": "gemini-3-flash-preview",
                "usage": {
                    "prompt_tokens": response_data["prompt_token_count"],
                    "completion_tokens": response_data["candidates_token_count"],
                    "total_tokens": response_data["total_token_count"],
                },
                "raw_request": request_payload,
                "raw_response": response_data,
            }
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ Gemini API: {e}")
            raise
    
    async def stream_explain(self, topic: str, level: str) -> AsyncGenerator[str, None]:
        """
        –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º.
        """
        if level not in LEVEL_PROMPTS:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {level}")
        
        system_prompt = LEVEL_PROMPTS[level]
        user_prompt = f"–û–±—ä—è—Å–Ω–∏ —Ç–µ–º—É: {topic}"
        
        print(f"\n[STREAM] –ó–∞–ø—Ä–æ—Å: —Ç–µ–º–∞='{topic}', —É—Ä–æ–≤–µ–Ω—å='{level}'\n")
        
        config = self._get_config(level)
        contents = f"{system_prompt}\n\n{user_prompt}"
        
        # –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        for chunk in self.client.models.generate_content_stream(
            model="gemini-3-flash-preview",
            contents=contents,
            config=config,
        ):
            if chunk.text:
                yield chunk.text
